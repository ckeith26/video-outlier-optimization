{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "pBeM8bpOqovJ"
   },
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "import transformers\n",
    "import math\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torchvision\n",
    "from torchvision.models import resnet101\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import DataLoader\n",
    "import av\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import UCF101\n",
    "import time\n",
    "import re\n",
    "import configparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "1atlL0GalJzU"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(config_file):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = read_config('../config.ini')\n",
    "except:\n",
    "    config = read_config('./config.ini')\n",
    "\n",
    "epochs = config.getint('training', 'epochs')\n",
    "train_subset = config.get('data', 'train_subset_100k')\n",
    "test_subset = config.get('data', 'test_subset_20k')\n",
    "train_subset_rest = config.get('data', 'train_subset_100k_rest')\n",
    "test_subset_rest = config.get('data', 'test_subset_20k_rest')\n",
    "target_epochs = config.getint('training', 'epochs')\n",
    "batch_size = config.getint('training', 'batch_size')\n",
    "train_size = config.getint('training', 'train_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "5r2u2Iw-lMIm"
   },
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    filtered_batch = []\n",
    "    for video, _, label in batch:\n",
    "        filtered_batch.append((video, label))\n",
    "    return torch.utils.data.dataloader.default_collate(filtered_batch)\n",
    "\n",
    "def custom_collate(batch):\n",
    "    filtered_batch = []\n",
    "    for video, _, label in batch:\n",
    "        filtered_batch.append((video, label))\n",
    "    return torch.utils.data.dataloader.default_collate(filtered_batch)\n",
    "def divide_by_255(x):\n",
    "            return x / 255.\n",
    "\n",
    "def permute_channels(x):\n",
    "    return x.permute(0, 3, 1, 2)\n",
    "\n",
    "def interpolate(x):\n",
    "    return nn.functional.interpolate(x, (240, 320))\n",
    "\n",
    "tfs = transforms.Compose([\n",
    "    transforms.Lambda(divide_by_255),\n",
    "    transforms.Lambda(permute_channels),\n",
    "    transforms.Lambda(interpolate),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_files is an array of data paths and is passed from executable setup.py\n",
    "\n",
    "# Load train_dataset\n",
    "with open('../data/train_dataset.pt', 'rb') as f:\n",
    "    train_dataset = torch.load(f)\n",
    "\n",
    "# Load test_dataset\n",
    "with open('../data/test_dataset.pt', 'rb') as f:\n",
    "    test_dataset = torch.load(f)\n",
    "\n",
    "# Load train_dataset_100k\n",
    "with open(train_subset[0], 'rb') as f:\n",
    "    train_set = torch.load(f)\n",
    "\n",
    "# Load test_dataset_20k\n",
    "with open(test_subset, 'rb') as f:\n",
    "    test_set = torch.load(f)\n",
    "\n",
    "# Load train_dataset_100k_rest\n",
    "with open(train_subset_rest, 'rb') as f:\n",
    "    test_set_org = torch.load(f)\n",
    "\n",
    "# Load test_dataset_20k_rest\n",
    "with open(test_subset_rest, 'rb') as f:\n",
    "    test_set2 = torch.load(f)\n",
    "    \n",
    "train_loader_sub = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
    "                                           collate_fn=custom_collate)\n",
    "test_loader_sub = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True,\n",
    "                                          collate_fn=custom_collate)\n",
    "# train_loader_full = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "#                                            collate_fn=custom_collate)\n",
    "# test_loader_full = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True,\n",
    "#                                           collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uuTN__kuCsRf",
    "outputId": "c4172c40-c4d1-42d4-d2a0-088654920647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of train samples: 1747933\n",
      "Total number of test samples: 682084\n",
      "Total number of (train) batches: 782\n",
      "Total number of (test) batches: 157\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of train samples: {len(train_dataset)}\")\n",
    "print(f\"Total number of test samples: {len(test_dataset)}\")\n",
    "print(f\"Total number of (train) batches: {len(train_loader_sub)}\")\n",
    "print(f\"Total number of (test) batches: {len(test_loader_sub)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "3GnvLKYPQTnF"
   },
   "outputs": [],
   "source": [
    "class HLSTMCNN(nn.Module):\n",
    "    def __init__(self, num_classes=101):\n",
    "        super(HLSTMCNN, self).__init__()\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Sequential(nn.Linear(self.resnet.fc.in_features, 300))\n",
    "        self.lstm = nn.LSTM(input_size=300, hidden_size=256, num_layers=3)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x_3d):\n",
    "        hidden = None\n",
    "\n",
    "        # Iterate over each frame of a video in a video of batch * frames * channels * height * width\n",
    "        for t in range(x_3d.size(1)):\n",
    "            with torch.no_grad():\n",
    "                x = self.resnet(x_3d[:, t])\n",
    "            # Pass latent representation of frame through lstm and update hidden state\n",
    "            out, hidden = self.lstm(x.unsqueeze(0), hidden)\n",
    "\n",
    "        # Get the last hidden state (hidden is a tuple with both hidden and cell state in it)\n",
    "        x = self.fc1(hidden[0][-1])\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hY6aMIPFiEvu",
    "outputId": "4d052e7a-d769-4d81-bc23-6ab09bf3f874"
   },
   "outputs": [],
   "source": [
    "device = device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")  # use gpu if available\n",
    "target_model = HLSTMCNN().to(device=device)\n",
    "optimiser=torch.optim.SGD(target_model.parameters(),lr=0.01,momentum=0.9)\n",
    "cost = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "4LVCyUtLiEyz"
   },
   "outputs": [],
   "source": [
    "\n",
    "def target_train_with_outlier_detection(train_loader, target_model, optimiser, val_out=0.05):\n",
    "   target_model.train()\n",
    "   size = len(train_loader.dataset)\n",
    "   correct = 0\n",
    "   total_loss=[]\n",
    "   total_sample=0\n",
    "   for batch, (X, Y) in enumerate(tqdm(train_loader)):\n",
    "       X, Y = X.to(device=device), Y.to(device=device)\n",
    "       #print(X.shape, Y.shape)\n",
    "       X_0_1=X[:,0, 0, :,:]\n",
    "       X_0_1 = X_0_1.reshape(X.shape[0], X_0_1.shape[1]*X_0_1.shape[2])/255.0\n",
    "       #print(X[0][0])\n",
    "\n",
    "       isf = IsolationForest(contamination=val_out, n_jobs=-1)\n",
    "       X_0_1=X_0_1.cpu().detach().numpy()\n",
    "       _ = isf.fit(X_0_1)\n",
    "\n",
    "\n",
    "       # Predictions\n",
    "       preds = isf.predict(X_0_1)\n",
    "       non_outl=X[preds!=-1]\n",
    "       new_X=non_outl\n",
    "    #    print(non_outl.shape)\n",
    "       #new_X[batch*batch_size_train + len(non_outl)-1]=new_X\n",
    "    #    print(f'Batch {batch} >> {len(non_outl)} Non-Outliers detected with contamination {val_out}')\n",
    "       outl_lab = Y[preds==-1]\n",
    "       nonoutl_lab = Y[preds!=-1]\n",
    "       new_Y=nonoutl_lab\n",
    "       total_sample=total_sample+new_X.shape[0]\n",
    "       optimiser.zero_grad()\n",
    "       pred = target_model(new_X)\n",
    "       #print(pred, Y)\n",
    "       #pred = pred.flatten()\n",
    "       loss = cost(pred, new_Y)\n",
    "       loss.backward()\n",
    "       optimiser.step()\n",
    "       _, output = torch.max(pred, 1)\n",
    "       #print(output, Y)\n",
    "       correct+= (output == new_Y).sum().item()\n",
    "       total_loss.append(loss.item())\n",
    "       #batch_count+=batch\n",
    "       #correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "\n",
    "   #print(correct, size)\n",
    "   correct /= size\n",
    "   loss= sum(total_loss)/(batch+1)\n",
    "#    print(\"Total Sample: \", total_sample)\n",
    "   result_train=100*correct\n",
    "   torch.save(target_model, '../models/base_model_100k_OD.pt')\n",
    "\n",
    "\n",
    "   print(f'\\nTraining Performance:\\nacc: {(100*correct):>0.1f}%, avg loss: {loss:>8f}\\n')\n",
    "  \n",
    "   return loss, result_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_utility_top5(test_loader, target_model, batch_size = batch_size):\n",
    "   size = len(test_loader.dataset)\n",
    "   target_model.eval()\n",
    "   test_loss, correct = 0, 0\n",
    "   correct = 0\n",
    "   topk_correct=0\n",
    "   total=0\n",
    "   counter_a=0\n",
    "   #with torch.no_grad():\n",
    "   for batch, (X, Y) in enumerate(tqdm(test_loader)):\n",
    "       X, Y = X.to(device=device), Y.to(device=device)\n",
    "       X.requires_grad = True\n",
    "       pred = target_model(X)\n",
    "       #print(\"Y is: \", Y)\n",
    "       counter_a=counter_a+1\n",
    "       #test_loss += cost(pred, Y).item()\n",
    "       #correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "       #data, target = data.to(device), target.to(device)\n",
    "       # Set requires_grad attribute of tensor. Important for Attack\n",
    "       total += Y.size(0)\n",
    "       # Forward pass the data through the model\n",
    "       _, output_res = torch.max(pred, -1)\n",
    "       #print(\"pred is: \",output_res)\n",
    "       correct += ((output_res) == Y).sum().item()\n",
    "       top_k=5\n",
    "       for test_sample in range(X.shape[0]):\n",
    "           val, idx=torch.topk(pred[test_sample, :], top_k)\n",
    "           for check in range(top_k):\n",
    "               if (idx[check]==Y[test_sample]):\n",
    "                   topk_correct+=1\n",
    "\n",
    "\n",
    "   # Calculate final accuracy for this epsilon\n",
    "   topk_acc= topk_correct/float(total)\n",
    "   final_acc = correct/float(total)\n",
    "   print(f\"Target Model Top-1 Acc = {correct} / {total} = {final_acc}, Top-5 Acc= {topk_correct} / {total}={topk_acc}\")\n",
    "\n",
    "\n",
    "   # Return the accuracy and an adversarial example\n",
    "   return final_acc, topk_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIKbLswRiE1v",
    "outputId": "51064573-e008-4682-ee19-f7c2591d4ad8"
   },
   "outputs": [],
   "source": [
    "loss_train_tr, loss_test_tr=[],[]\n",
    "for t in tqdm(range(target_epochs)):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    print(\"+++++++++Target Training Starting+++++++++\")\n",
    "    tr_loss, result_train=target_train_with_outlier_detection(train_loader_sub, target_model, optimiser)\n",
    "    loss_train_tr.append(tr_loss)\n",
    "\n",
    "    # final_acc, topk_acc=target_utility_top5(test_loader_sub, target_model, batch_size = batch_size)\n",
    "\n",
    "    print(\n",
    "    f'CNN Training update after {t+1} epochs',\n",
    "    f'''\n",
    "        Train loss: {tr_loss}\n",
    "        Train accuracy: {round(result_train/100, 2)}\n",
    "     '''\n",
    "        # Top-1 Test accuracy: {final_acc}\n",
    "        # Top-5 Test accuracy: {topk_acc}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours =  (time.time()-start_time)//3600\n",
    "minutes = ((time.time()-start_time)%3600)//60\n",
    "seconds = ((time.time()-start_time)%3600)%60\n",
    "date = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcqfrO5COQCe",
    "outputId": "4b56e2b5-2ea9-4c05-a5f3-8e49d7e3e7a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++Target Test+++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [20:33<00:00,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Model Accuracy = 14652 / 25000 = 0.58608\n",
      "Test Acc: 0.58608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_acc_top1, final_acc_top5=target_utility_top5(test_loader_sub, target_model, batch_size = batch_size)\n",
    "# save to a file\n",
    "\n",
    "with open('../results/results.txt', 'w') as f:\n",
    "    f.write(f\"\\n----------------------------------------------\\n\")\n",
    "    f.write(f\"Training Finished at: {date}\\n\")\n",
    "    f.write(f\"Base Model Training Samples: {train_size}\\n\")\n",
    "    f.write(f\"Final Train Loss: {round(loss_train_tr[-1], 2)}\\n\")\n",
    "    f.write(f\"Final Test Loss: {round(loss_test_tr[-1], 2)}\\n\")\n",
    "    f.write(f\"Final Top-1 Accuracy: {final_acc_top1}\\n\")\n",
    "    f.write(f\"Final Top-5 Accuracy: {final_acc_top5}\\n\")\n",
    "    f.write(f\"Total Training Time: {hours} Hours, {minutes} Minutes, {seconds} Seconds\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
